---
title: "HW3"
output: html_document
---

``` {r}
library(tidymodels)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) 
library(dplyr)
library(MASS)
library(pROC)
tidymodels_prefer()

library(readr)
titanic <- read_csv("~/Downloads/homework-3/data/titanic.csv")
View(titanic)
```
# Question 1

```{r}

titanic$survived <- factor(titanic$survived, levels = c("Yes","No"))
titanic$pclass <- as.factor(titanic$pclass)

set.seed(891)

titanic_split <- initial_split(titanic, prop = 0.8, strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```
Training set observations: 712
Testing set observations: 179

Some variables have missing data, which is why we will used stratified sampling.

It is a good idea to use stratified sampling for this data because it provides better coverage of the population as the researchers have control over the subgroups to ensure all of them are represented in the sampling.

# Question 2
```{r}
library(ggplot2)
ggplot(titanic_train, aes(x=survived)) +
  geom_bar(fill='red') +  labs(x='Survived')

```
The variable survived has two levels: Yes and No. From the distribution of the training data set, we can see that more individuals died than survived. The count of those who survived is a little less than 300, while the count for those who passed is greater than 400.

# Question 3
```{r}
cor_titanic <- titanic_train %>%
  select(passenger_id,age,sib_sp,parch,fare) %>%
  correlate()
rplot(cor_titanic)

cor_titanic %>%
  stretch() %>%
  ggplot(aes(x, y, fill = r)) +
  geom_tile() +
  geom_text(aes(label = as.character(fashion(r))))
```
From the two matrices, we can see that only sib_sp and parch have much of any correlation to one another in a positive direction, and it's only about .40.

# Question 4
```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~starts_with("sex"):fare) %>%
  step_interact(~age:fare) %>%
  step_normalize(all_predictors())

```

# Question 5
```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

log_fit <- fit(log_wkflow, titanic_train)

log_fit %>% 
  tidy()

```

# Question 6
```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, titanic_train)

```

# Question 7
```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_wkflow, titanic_train)
```

# Question 8
```{r}
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titanic_recipe)

nb_fit <- fit(nb_wkflow, titanic_train)
```

# Question 9
```{r}
#predictions
log_p <- predict(log_fit, new_data = titanic_train, type = "prob")
log_p

lda_p <- predict(lda_fit, new_data = titanic_train, type = "prob")
lda_p

qda_p <- predict(qda_fit, new_data = titanic_train, type = "prob")
qda_p

nb_p <- predict(nb_fit, new_data = titanic_train, type = "prob")
nb_p

#accuracies
log_reg_acc <- augment(log_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
log_reg_acc

lda_acc <- augment(lda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
lda_acc

qda_acc <- augment(qda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
qda_acc

nb_acc <- augment(nb_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
nb_acc

#through bind_col
titanic_train_res <- bind_cols(log_p, lda_p, qda_p, nb_p, titanic_train %>% select(survived))
titanic_train_res %>% 
  head()

#comparing model performance through accuracy test
accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```
The model that achieved the highest accuracy was the Logistic Regression model.

# Question 10
```{r}
predict(log_fit, new_data = titanic_test, type = "prob")

multi_metric <- metric_set(accuracy, sensitivity, specificity)

augment(log_fit, new_data = titanic_test) %>%
  multi_metric(truth = survived, estimate = .pred_class)

augment(log_fit, new_data = titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

augment(log_fit, new_data = titanic_test) %>%
  roc_curve(survived, .pred_Yes) %>%
  autoplot()

augment(log_fit, new_data = titanic_test) %>%
  roc_auc(survived,.pred_Yes)

```
In terms of model performance, the AUC from the ROC curve gives us a .85 estimate. This indicates that the performance essentially earned a "B" if we were to interpret it as a letter grade. 

In comparison to the training data, the former had an accuracy about .814, while the testing data had an accuracy about .810. The two values are basically the same, but still slightly differ, and that is because the test accuracy should not be higher than the train as the model is optimized for the latter.

